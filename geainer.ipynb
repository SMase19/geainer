{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import six\n",
    "\n",
    "import chainer\n",
    "from chainer import computational_graph\n",
    "from chainer import cuda\n",
    "from chainer import optimizers\n",
    "from chainer import serializers\n",
    "from chainer import functions as f\n",
    "from chainer import links as l\n",
    "from chainer import initializer, cuda\n",
    "from chainer.functions.loss.vae import gaussian_kl_divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 5000)\n",
      "float32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-02-0047-01</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-02-0055-01</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A  \\\n",
       "TCGA-02-0047-01  0.678296  0.289910  0.034230   0.0      0.0  0.084731   \n",
       "TCGA-02-0055-01  0.200633  0.654917  0.181993   0.0      0.0  0.100606   \n",
       "\n",
       "                    KRT14   CEACAM6     DDX3Y     KDM5D    ...      FAM129A  \\\n",
       "TCGA-02-0047-01  0.031863  0.037709  0.746797  0.687832    ...     0.440610   \n",
       "TCGA-02-0055-01  0.050011  0.092586  0.103725  0.140642    ...     0.620658   \n",
       "\n",
       "                  C8orf48    CDK5R1    FAM81A  C13orf18     GDPD3     SMAGP  \\\n",
       "TCGA-02-0047-01  0.428782  0.732819  0.634340  0.580662  0.294313  0.458134   \n",
       "TCGA-02-0055-01  0.363207  0.592269  0.602755  0.610192  0.374569  0.722420   \n",
       "\n",
       "                  C2orf85   POU5F1B     CHST2  \n",
       "TCGA-02-0047-01  0.478219  0.168263  0.638497  \n",
       "TCGA-02-0055-01  0.271356  0.160465  0.602560  \n",
       "\n",
       "[2 rows x 5000 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 遺伝子発現量RNAseqを読み込む\n",
    "rnaseq_file = os.path.join('data', 'pancan_scaled_zeroone_rnaseq.tsv')\n",
    "rnaseq_df = pd.read_table(rnaseq_file, index_col=0).astype(np.float32)\n",
    "print(rnaseq_df.shape)\n",
    "print(rnaseq_df.values[0].dtype)\n",
    "rnaseq_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq_df.sample(frac=test_set_percent).astype(np.float32)\n",
    "rnaseq_train_df = rnaseq_df.drop(rnaseq_test_df.index).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set hyper parameters\n",
    "original_dim = rnaseq_df.shape[1]\n",
    "latent_dim = 100\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "learning_rate = 0.0005\n",
    "\n",
    "epsilon_std = 1.0\n",
    "kappa = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Xavier(initializer.Initializer):\n",
    "    \"\"\"\n",
    "    Xavier initializaer\n",
    "    Reference:\n",
    "    * https://jmetzen.github.io/2015-11-27/vae.html\n",
    "    * https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, constant=1, dtype=None):\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "        self.high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        self.low = -self.high\n",
    "        super(Xavier, self).__init__(dtype)\n",
    "\n",
    "    def __call__(self, array):\n",
    "        xp = cuda.get_array_module(array)\n",
    "        args = {'low': self.low, 'high': self.high, 'size': array.shape}\n",
    "        if xp is not np:\n",
    "            # Only CuPy supports dtype option\n",
    "            if self.dtype == np.float32 or self.dtype == np.float16:\n",
    "                # float16 is not supported in cuRAND\n",
    "                args['dtype'] = np.float32\n",
    "        array[...] = xp.random.uniform(**args)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(chainer.Chain):\n",
    "    \"\"\"Variational AutoEncoder\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_latent, n_h, act_func=f.tanh):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.act_func = act_func\n",
    "        with self.init_scope():\n",
    "            # encoder\n",
    "            self.le1 = l.Linear(n_in,       n_h,        initialW=Xavier(n_in, n_h))\n",
    "            self.le2 = l.Linear(n_h,        n_h,        initialW=Xavier(n_h, n_h))\n",
    "            self.bn1 = l.BatchNormalization(n_h)\n",
    "            self.le3_mu = l.Linear(n_h,     n_latent,   initialW=Xavier(n_h,  n_latent))\n",
    "            self.le3_ln_var = l.Linear(n_h, n_latent,   initialW=Xavier(n_h,  n_latent))\n",
    "            # decoder\n",
    "            self.ld1 = l.Linear(n_latent,   n_h,        initialW=Xavier(n_latent, n_h))\n",
    "            self.ld2 = l.Linear(n_h,        n_h,        initialW=Xavier(n_h, n_h))\n",
    "            self.ld3 = l.Linear(n_h,        n_in,       initialW=Xavier(n_h, n_in))\n",
    "            self.ld4 = l.Linear(n_latent,   n_in,       initialW=Xavier(n_latent, n_in))\n",
    "\n",
    "    def __call__(self, x, sigmoid=True):\n",
    "        \"\"\"AutoEncoder\"\"\"\n",
    "        return self.decode(self.encode(x)[0], sigmoid)\n",
    "\n",
    "    def encode(self, x):\n",
    "        if type(x) != chainer.variable.Variable:\n",
    "            x = chainer.Variable(x)\n",
    "        x.name = \"x\"\n",
    "        # h1 = self.act_func(l.BatchNormalization(self.le1(x)))\n",
    "        h1 =self.act_func( self.le1(x))\n",
    "        h1.name = \"enc_h1\"\n",
    "        h1_2 = h1#self.bn1(h1)\n",
    "        \n",
    "        h2 = self.act_func(self.le2(h1_2))\n",
    "        h2.name = \"enc_h2\"\n",
    "        mu = self.le3_mu(h2)\n",
    "        mu.name = \"z_mu\"\n",
    "        ln_var = self.le3_ln_var(h2)  # ln_var = log(sigma**2)\n",
    "        ln_var.name = \"z_ln_var\"\n",
    "        return mu, ln_var\n",
    "\n",
    "    def decode(self, z, sigmoid=True):\n",
    "        # h1 = self.act_func(self.ld1(z))\n",
    "        # h1.name = \"dec_h1\"\n",
    "        # h2 = self.act_func(self.ld2(h1))\n",
    "        # h2.name = \"dec_h2\"\n",
    "        # h3 = self.ld3(h2)\n",
    "        # h3.name = \"dec_h3\"\n",
    "        h4 = self.ld4(z)\n",
    "        h4.name = \"dec_h4\"\n",
    "        if sigmoid:\n",
    "            return f.sigmoid(h4)\n",
    "        else:\n",
    "            return h4\n",
    "\n",
    "    def get_loss_func(self, C=1.0, k=1):\n",
    "        \"\"\"Get loss function of VAE.\n",
    "        The loss value is equal to ELBO (Evidence Lower Bound)\n",
    "        multiplied by -1.\n",
    "        Args:\n",
    "            C (int): Usually this is 1.0. Can be changed to control the\n",
    "                second term of ELBO bound, which works as regularization.\n",
    "            k (int): Number of Monte Carlo samples used in encoded vector.\n",
    "        \"\"\"\n",
    "        def lf(x):\n",
    "            mu, ln_var = self.encode(x)\n",
    "            batch_size = len(mu.data)\n",
    "            # reconstruction loss\n",
    "            rec_loss = 0\n",
    "            for l in range(k):\n",
    "                z = f.gaussian(mu, ln_var)\n",
    "                z.name = \"z\"\n",
    "                rec_loss += f.bernoulli_nll(x, self.decode(z, sigmoid=True)) \\\n",
    "                    / (k * batch_size)\n",
    "            self.rec_loss = rec_loss\n",
    "            self.rec_loss.name = \"reconstruction error\"\n",
    "            self.latent_loss = C * gaussian_kl_divergence(mu, ln_var) / batch_size\n",
    "            self.latent_loss.name = \"latent loss\"\n",
    "            self.loss = self.rec_loss + self.latent_loss\n",
    "            self.loss.name = \"loss\"\n",
    "            return self.loss\n",
    "        return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare VAE model\n",
    "model = VariationalAutoEncoder(original_dim, latent_dim, latent_dim, act_func=f.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = np\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.Adam(alpha=learning_rate)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# N = rnaseq_train_df.shape[0]\n",
    "# N_test = rnaseq_test_df.shape[0]\n",
    "N = 9000\n",
    "x_train, x_test = np.split(rnaseq_df,   [N])\n",
    "print(x_train.values[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7295000\n"
     ]
    }
   ],
   "source": [
    "N_test = x_test.size\n",
    "print(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train mean loss=3496.5251559787325, mean reconstruction loss=3465.932923719618\n",
      "epoch 2\n",
      "train mean loss=3481.5491644965277, mean reconstruction loss=3457.869504123264\n",
      "epoch 3\n",
      "train mean loss=3471.83759765625, mean reconstruction loss=3450.4716851128474\n",
      "epoch 4\n",
      "train mean loss=3465.675351291233, mean reconstruction loss=3446.482469346788\n",
      "epoch 5\n",
      "train mean loss=3461.988062879774, mean reconstruction loss=3444.2850992838544\n",
      "epoch 6\n",
      "train mean loss=3458.95608859592, mean reconstruction loss=3442.571861436632\n",
      "epoch 7\n",
      "train mean loss=3456.8594767252603, mean reconstruction loss=3440.8762953016494\n",
      "epoch 8\n",
      "train mean loss=3454.1037651909724, mean reconstruction loss=3438.6315199110245\n",
      "epoch 9\n",
      "train mean loss=3452.1212239583333, mean reconstruction loss=3436.969173177083\n",
      "epoch 10\n",
      "train mean loss=3450.7045925564234, mean reconstruction loss=3435.912411838108\n",
      "epoch 11\n",
      "train mean loss=3449.8045328776043, mean reconstruction loss=3435.1784152560763\n",
      "epoch 12\n",
      "train mean loss=3448.3649102105037, mean reconstruction loss=3434.202574327257\n",
      "epoch 13\n",
      "train mean loss=3447.6008395724825, mean reconstruction loss=3433.5289442274307\n",
      "epoch 14\n",
      "train mean loss=3446.4534288194445, mean reconstruction loss=3432.6049126519097\n",
      "epoch 15\n",
      "train mean loss=3445.7031290690106, mean reconstruction loss=3432.019253200955\n",
      "epoch 16\n",
      "train mean loss=3445.2650146484375, mean reconstruction loss=3431.5774726019963\n",
      "epoch 17\n",
      "train mean loss=3444.6474310980902, mean reconstruction loss=3431.0241034613714\n",
      "epoch 18\n",
      "train mean loss=3443.99889187283, mean reconstruction loss=3430.519007703993\n",
      "epoch 19\n",
      "train mean loss=3443.5754909939237, mean reconstruction loss=3429.9742323133682\n",
      "epoch 20\n",
      "train mean loss=3442.8881876627606, mean reconstruction loss=3429.4800957573784\n",
      "epoch 21\n",
      "train mean loss=3442.1083889431425, mean reconstruction loss=3428.8160400390625\n",
      "epoch 22\n",
      "train mean loss=3441.7306966145834, mean reconstruction loss=3428.470841471354\n",
      "epoch 23\n",
      "train mean loss=3441.2475558810766, mean reconstruction loss=3428.019064670139\n",
      "epoch 24\n",
      "train mean loss=3441.2923475477432, mean reconstruction loss=3427.729387749566\n",
      "epoch 25\n",
      "train mean loss=3440.420465766059, mean reconstruction loss=3427.1320190429688\n",
      "epoch 26\n",
      "train mean loss=3440.2502143012152, mean reconstruction loss=3426.8837687174478\n",
      "epoch 27\n",
      "train mean loss=3440.0633626302083, mean reconstruction loss=3426.58134765625\n",
      "epoch 28\n",
      "train mean loss=3439.6153767903647, mean reconstruction loss=3426.1886962890626\n",
      "epoch 29\n",
      "train mean loss=3438.9260023328993, mean reconstruction loss=3425.654985894097\n",
      "epoch 30\n",
      "train mean loss=3438.8132364908856, mean reconstruction loss=3425.3195814344617\n",
      "epoch 31\n",
      "train mean loss=3438.522028266059, mean reconstruction loss=3425.048262532552\n",
      "epoch 32\n",
      "train mean loss=3437.9264200846355, mean reconstruction loss=3424.492970106337\n",
      "epoch 33\n",
      "train mean loss=3437.545429144965, mean reconstruction loss=3424.1501763237848\n",
      "epoch 34\n",
      "train mean loss=3437.065791829427, mean reconstruction loss=3423.708685980903\n",
      "epoch 35\n",
      "train mean loss=3436.663538953993, mean reconstruction loss=3423.2820963541667\n",
      "epoch 36\n",
      "train mean loss=3436.61842719184, mean reconstruction loss=3423.1680555555554\n",
      "epoch 37\n",
      "train mean loss=3436.486699761285, mean reconstruction loss=3422.9054646809896\n",
      "epoch 38\n",
      "train mean loss=3435.9093519422745, mean reconstruction loss=3422.4237887912327\n",
      "epoch 39\n",
      "train mean loss=3435.6093627929686, mean reconstruction loss=3422.0383395724825\n",
      "epoch 40\n",
      "train mean loss=3435.3880764431424, mean reconstruction loss=3421.8124321831597\n",
      "epoch 41\n",
      "train mean loss=3434.863240559896, mean reconstruction loss=3421.317264811198\n",
      "epoch 42\n",
      "train mean loss=3434.5181993272568, mean reconstruction loss=3420.8100273980035\n",
      "epoch 43\n",
      "train mean loss=3434.028903537326, mean reconstruction loss=3420.447713216146\n",
      "epoch 44\n",
      "train mean loss=3433.789438205295, mean reconstruction loss=3420.0776245117186\n",
      "epoch 45\n",
      "train mean loss=3433.559351942274, mean reconstruction loss=3419.850385199653\n",
      "epoch 46\n",
      "train mean loss=3433.26939968533, mean reconstruction loss=3419.6333984375\n",
      "epoch 47\n",
      "train mean loss=3432.8848741319443, mean reconstruction loss=3419.308390299479\n",
      "epoch 48\n",
      "train mean loss=3432.772741699219, mean reconstruction loss=3419.1829142252604\n",
      "epoch 49\n",
      "train mean loss=3432.5973253038196, mean reconstruction loss=3418.9527791341147\n",
      "epoch 50\n",
      "train mean loss=3432.309898546007, mean reconstruction loss=3418.72398952908\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# total_losses = np.zeros(epochs, dtype=np.float32)\n",
    "\n",
    "for epoch in six.moves.range(1, epochs + 1):\n",
    "    print('epoch', epoch)\n",
    "    # epoch_time = time.time()\n",
    "    \n",
    "    # training\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_loss = 0       # total loss\n",
    "    sum_rec_loss = 0   # reconstruction loss\n",
    "    \n",
    "    for i in six.moves.range(0, N, batch_size):\n",
    "        x = chainer.Variable(xp.asarray(x_train.values[perm[i:i + batch_size]]))\n",
    "        optimizer.update(model.get_loss_func(), x)\n",
    "        sum_loss += float(model.loss.data) * len(x.data)\n",
    "        sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n",
    "    \n",
    "    print('train mean loss={}, mean reconstruction loss={}'\n",
    "          .format(sum_loss / N, sum_rec_loss / N))\n",
    "    \n",
    "#     # evaluation\n",
    "#     sum_loss = 0\n",
    "#     sum_rec_loss = 0\n",
    "#     with chainer.no_backprop_mode():\n",
    "#         for i in six.moves.range(0, N_test, batch_size):\n",
    "#             x = chainer.Variable(xp.asarray(x_test.values[i:i + batch_size]))\n",
    "#             loss_func = model.get_loss_func(k=10)\n",
    "#             loss_func(x)\n",
    "#             sum_loss += float(model.loss.data) * len(x.data)\n",
    "#             sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n",
    "#             del model.loss\n",
    "#     print('test  mean loss={}, mean reconstruction loss={}'\n",
    "#           .format(sum_loss / N_test, sum_rec_loss / N_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1854cbf4ed9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msum_rec_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m   \u001b[0;31m# reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgraph_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Learning loop\n",
    "loss_list_train = []\n",
    "loss_list_test = []\n",
    "N=600000\n",
    "\n",
    "for epoch in six.moves.range(1, epochs + 1):\n",
    "    print('epoch', epoch)\n",
    "\n",
    "    # training\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_loss = 0       # total loss\n",
    "    sum_rec_loss = 0   # reconstruction loss\n",
    "    \n",
    "    for t in xrange(num_trains_per_epoch):\n",
    "    \n",
    "    \n",
    "    for i in six.moves.range(0, N, batch_size):\n",
    "        x = chainer.Variable(xp.asarray(rnaseq_train_df[perm[i:i + batchsize]]))\n",
    "        optimizer.update(model.get_loss_func(), x)\n",
    "        if epoch == 1 and i == 0 and graph_gen:\n",
    "            graph_export(model)\n",
    "\n",
    "        sum_loss += float(model.loss.data) * len(x.data)\n",
    "        sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n",
    "    loss_train = sum_loss / N\n",
    "    print('train mean loss={}, mean reconstruction loss={}'.format(sum_loss / N, sum_rec_loss / N))\n",
    "\n",
    "    # evaluation\n",
    "    sum_loss = 0\n",
    "    sum_rec_loss = 0\n",
    "    with chainer.no_backprop_mode():\n",
    "        for i in six.moves.range(0, N_test, batchsize):\n",
    "            x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n",
    "            loss_func = model.get_loss_func(k=10)\n",
    "            loss_func(x)\n",
    "            sum_loss += float(model.loss.data) * len(x.data)\n",
    "            sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n",
    "            del model.loss\n",
    "    print('test  mean loss={}, mean reconstruction loss={}'\n",
    "          .format(sum_loss / N_test, sum_rec_loss / N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
